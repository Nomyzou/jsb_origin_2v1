import numpy as np
import torch
import os
import sys
import time
from pathlib import Path
from datetime import datetime
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from collections import defaultdict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from envs.JSBSim.envs import MultipleCombatEnv
from algorithms.ppo.ppo_actor import PPOActor
from envs.JSBSim.utils.situation_assessment import get_situation_adv
from envs.JSBSim.utils.utils import LLA2NEU, get_AO_TA_R
from envs.JSBSim.core.catalog import Catalog as c
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡ç”¨äºè®°å½•æ•°æ®
target_matching_history = []

# ä¼¤å®³è®¡ç®—ç›¸å…³å¸¸é‡
DAMAGE_DISTANCE_THRESHOLD = 3000.0  # ä¼¤å®³è·ç¦»é˜ˆå€¼ (ç±³) - é™ä½åˆ°3å…¬é‡Œ
DAMAGE_ANGLE_THRESHOLD = np.pi/3     # ä¼¤å®³è§’åº¦é˜ˆå€¼ (60åº¦ï¼Œå¼§åº¦) - é™ä½è§’åº¦è¦æ±‚
MAX_DAMAGE_PER_STEP = 1.0           # æ¯æ­¥æœ€å¤§ä¼¤å®³å€¼ - é™ä½åˆ°5
DAMAGE_BASE_RATE = 2              # åŸºç¡€ä¼¤å®³ç‡ - é™ä½åŸºç¡€ä¼¤å®³

class Args:
    def __init__(self) -> None:
        self.gain = 0.01
        self.hidden_size = '128 128'
        self.act_hidden_size = '128 128'
        self.activation_id = 1
        self.use_feature_normalization = False
        self.use_recurrent_policy = True
        self.recurrent_hidden_size = 128
        self.recurrent_hidden_layers = 1
        self.tpdv = dict(dtype=torch.float32, device=torch.device('cpu'))
        self.use_prior = True
    
def _t2n(x):
    return x.detach().cpu().numpy()

def load_model_safely(model_path, policy, device):
    """å®‰å…¨åŠ è½½æ¨¡å‹æƒé‡"""
    try:
        if os.path.exists(model_path):
            state_dict = torch.load(model_path, map_location=device)
            policy.load_state_dict(state_dict)
            logger.info(f"Successfully loaded model from {model_path}")
            return True
        else:
            logger.warning(f"Model file not found: {model_path}")
            return False
    except Exception as e:
        logger.error(f"Failed to load model from {model_path}: {e}")
        return False

def calculate_damage(ego_feature, enm_feature, distance_threshold=DAMAGE_DISTANCE_THRESHOLD, 
                    angle_threshold=DAMAGE_ANGLE_THRESHOLD, base_rate=DAMAGE_BASE_RATE, debug=False):
    """
    è®¡ç®—åŸºäºè§’åº¦å’Œè·ç¦»çš„ä¼¤å®³å€¼ - ä¿®æ­£ç‰ˆæœ¬
    
    Args:
        ego_feature: å·±æ–¹é£æœºç‰¹å¾ (north, east, down, vn, ve, vd)
        enm_feature: æ•Œæ–¹é£æœºç‰¹å¾ (north, east, down, vn, ve, vd)
        distance_threshold: ä¼¤å®³è·ç¦»é˜ˆå€¼ (ç±³)
        angle_threshold: ä¼¤å®³è§’åº¦é˜ˆå€¼ (å¼§åº¦ï¼Œé»˜è®¤60åº¦)
        base_rate: åŸºç¡€ä¼¤å®³ç‡
    
    Returns:
        damage: æˆ‘æ–¹å¯¹æ•Œæ–¹çš„ä¼¤å®³å€¼ (0-MAX_DAMAGE_PER_STEPä¹‹é—´)
    """
    try:
        # è®¡ç®—AOå’ŒTAè§’åº¦ä»¥åŠè·ç¦»
        AO, TA, R, _ = get_AO_TA_R(ego_feature, enm_feature, return_side=True)
        if debug :
            logger.info(f"   è·ç¦»: {R:.2f}m ({R/1000:.2f}km)")
            logger.info(f"   AOè§’åº¦: {np.degrees(AO):.2f}Â°")
            logger.info(f"   TAè§’åº¦: {np.degrees(TA):.2f}Â°")
        # æ£€æŸ¥æ˜¯å¦æ»¡è¶³ä¼¤å®³æ¡ä»¶
        if R > distance_threshold:
            if debug :
                logger.info(f"   è·ç¦»è¶…è¿‡é˜ˆå€¼: {R:.2f}m ({R/1000:.2f}km)")
            return 0.0
        
        # æ­£ç¡®çš„ä¼¤å®³è®¡ç®—é€»è¾‘ï¼š
        # å½“AO < angle_thresholdæ—¶ï¼Œæˆ‘æ–¹å¯¹æ•Œæ–¹é€ æˆä¼¤å®³
        # AOè¶Šå°ä¼¤å®³è¶Šå¤§ï¼ŒTAè¶Šå¤§ä¼¤å®³è¶Šå¤§
        
        if AO < angle_threshold:  # æˆ‘æ–¹æœå‘æ•Œæœºï¼ˆAO < 60åº¦ï¼‰
            # AOè§’åº¦å› å­ï¼šAOè¶Šå°ï¼Œä¼¤å®³è¶Šå¤§
            ao_factor = 1.0 - (AO / angle_threshold)
            ao_factor = np.clip(ao_factor, 0, 1)
            
            # TAè§’åº¦å› å­ï¼šTAè¶Šå¤§ï¼Œä¼¤å®³è¶Šå¤§ï¼ˆæ•Œæœºè¶Šéš¾é€ƒè„±ï¼‰
            # TAä»0åˆ°Ï€ï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨TAè¾ƒå¤§æ—¶ä¼¤å®³æ›´å¤§
            ta_factor = TA / np.pi
            ta_factor = np.clip(ta_factor, 0, 1)
            
            # ç»¼åˆè§’åº¦å› å­
            angle_factor = ao_factor * ta_factor
        else:
            angle_factor = 0.0
        
        # è®¡ç®—åŸºäºè·ç¦»çš„ä¼¤å®³ (è·ç¦»è¶Šè¿‘ä¼¤å®³è¶Šå¤§)
        distance_factor = 3.0 - (R / distance_threshold)
        distance_factor = np.clip(distance_factor, 0, 10)
        
        # ç»¼åˆä¼¤å®³è®¡ç®—
        damage = base_rate * angle_factor * distance_factor
        damage = np.clip(damage, 0, MAX_DAMAGE_PER_STEP)
        if debug :
            logger.info(f"   ğŸ¯ æœ€ç»ˆä¼¤å®³: {damage:.4f}")
        
        return damage
        
    except Exception as e:
        logger.error(f"Error calculating damage: {e}")
        return 0.0

def apply_damage_system(env, center_lon=120.0, center_lat=60.0, center_alt=0.0):
    """
    åº”ç”¨ä¼¤å®³ç³»ç»Ÿï¼šè®¡ç®—å¹¶åº”ç”¨æ‰€æœ‰é£æœºä¹‹é—´çš„ä¼¤å®³
    
    Args:
        env: ç¯å¢ƒå¯¹è±¡
        center_lon, center_lat, center_alt: æˆ˜åœºä¸­å¿ƒåæ ‡
    
    Returns:
        damage_log: ä¼¤å®³æ—¥å¿—ä¿¡æ¯
    """
    try:
        damage_log = []
        state_var = env.task.state_var
        
        # è·å–æ‰€æœ‰é£æœºID
        all_agent_ids = list(env.agents.keys())
        red_agent_ids = [aid for aid in all_agent_ids if aid.startswith('A')]
        blue_agent_ids = [aid for aid in all_agent_ids if aid.startswith('B')]
        
        # åˆå§‹åŒ–ä¼¤å®³ç´¯ç§¯
        damage_to_red = {red_id: 0.0 for red_id in red_agent_ids}
        damage_to_blue = {blue_id: 0.0 for blue_id in blue_agent_ids}
        
        # ä¸€æ¬¡éå†ï¼šè®¡ç®—æ‰€æœ‰é£æœºå¯¹å½¼æ­¤çš„ä¼¤å®³
        for red_id in red_agent_ids:
            if not env.agents[red_id].is_alive:
                continue
                
            red_agent = env.agents[red_id]
            red_state = red_agent.get_property_values(state_var)
            red_cur_ned = LLA2NEU(*red_state[:3], center_lon, center_lat, center_alt)
            red_feature = np.array([*red_cur_ned, *(red_state[6:9])])
            
            for blue_id in blue_agent_ids:
                if not env.agents[blue_id].is_alive:
                    continue
                    
                blue_agent = env.agents[blue_id]
                blue_state = blue_agent.get_property_values(state_var)
                blue_cur_ned = LLA2NEU(*blue_state[:3], center_lon, center_lat, center_alt)
                blue_feature = np.array([*blue_cur_ned, *(blue_state[6:9])])
                
                # è®¡ç®—è·ç¦»
                distance = np.linalg.norm([blue_cur_ned[0] - red_cur_ned[0], 
                                         blue_cur_ned[1] - red_cur_ned[1], 
                                         blue_cur_ned[2] - red_cur_ned[2]])
                
                # è®¡ç®—æˆ‘æ–¹å¯¹æ•Œæ–¹çš„ä¼¤å®³
                red_to_blue_damage = calculate_damage(red_feature, blue_feature,debug=True)
                if red_to_blue_damage > 0:
                    damage_to_blue[blue_id] += red_to_blue_damage
                    damage_log.append({
                        'attacker': red_id,
                        'target': blue_id,
                        'damage': red_to_blue_damage,
                        'type': 'AO_damage',
                        'distance': distance
                    })
                
                # è®¡ç®—æ•Œæ–¹å¯¹æˆ‘æ–¹çš„ä¼¤å®³
                blue_to_red_damage = calculate_damage(blue_feature, red_feature)
                if blue_to_red_damage > 0:
                    damage_to_red[red_id] += blue_to_red_damage
                    damage_log.append({
                        'attacker': blue_id,
                        'target': red_id,
                        'damage': blue_to_red_damage,
                        'type': 'TA_damage',
                        'distance': distance
                    })
        
        # åº”ç”¨ä¼¤å®³åˆ°æ‰€æœ‰é£æœº
        for red_id, damage in damage_to_red.items():
            if damage > 0 and env.agents[red_id].is_alive:
                env.agents[red_id].bloods = max(0, env.agents[red_id].bloods - damage)
                if env.agents[red_id].bloods <= 0:
                    env.agents[red_id].is_alive = False
                    logger.info(f"{red_id} è¢«å‡»è½! æ€»ä¼¤å®³: {damage:.4f}")
        
        for blue_id, damage in damage_to_blue.items():
            if damage > 0 and env.agents[blue_id].is_alive:
                env.agents[blue_id].bloods = max(0, env.agents[blue_id].bloods - damage)
                if env.agents[blue_id].bloods <= 0:
                    env.agents[blue_id].is_alive = False
                    logger.info(f"{blue_id} è¢«å‡»è½! æ€»ä¼¤å®³: {damage:.4f}")
        
        return damage_log
        
    except Exception as e:
        logger.error(f"Error in damage system: {e}")
        return []

def select_best_target(ego_agent_id, env, center_lon=120.0, center_lat=60.0, center_alt=0.0):
    """
    ä¸ºæŒ‡å®šæˆ‘æ–¹é£æœºé€‰æ‹©æœ€ä¼˜æ•Œæ–¹ç›®æ ‡ - ç®€åŒ–ç‰ˆæœ¬
    é€šè¿‡è®¡ç®—åŒå‘æ€åŠ¿ä¼˜åŠ¿å‡½æ•°ï¼Œé€‰æ‹©ä¼˜åŠ¿å·®å€¼æœ€å¤§çš„æ•Œæ–¹é£æœº
    å…è®¸å¤šæ¶æˆ‘æ–¹é£æœºé€‰æ‹©åŒä¸€ä¸ªç›®æ ‡
    """
    try:
        # è·å–æˆ‘æ–¹é£æœºå¯¹è±¡
        ego_agent = env.agents[ego_agent_id]
        
        # è·å–æ‰€æœ‰æ•Œæ–¹é£æœºID
        enemy_ids = []
        for agent_id in env.agents.keys():
            if agent_id.startswith('B') and env.agents[agent_id].is_alive:  # åªé€‰æ‹©å­˜æ´»çš„æ•Œæ–¹é£æœº
                enemy_ids.append(agent_id)
        
        if not enemy_ids:
            logger.warning(f"No alive enemy agents found for {ego_agent_id}")
            return None
        
        # è·å–çŠ¶æ€å˜é‡
        state_var = env.task.state_var
        
        # è®¡ç®—åŒå‘æ€åŠ¿ä¼˜åŠ¿
        advantage_differences = {}
        advantage_details = {}
        for enemy_id in enemy_ids:
            try:
                enemy_agent = env.agents[enemy_id]
                
                # è·å–çŠ¶æ€å€¼
                ego_state = ego_agent.get_property_values(state_var)
                enemy_state = enemy_agent.get_property_values(state_var)
                
                # è®¡ç®—æˆ‘æ–¹å¯¹æ•Œæ–¹çš„ä¼˜åŠ¿ï¼ˆå¯¹æˆ‘æ–¹æœ‰åˆ©çš„å€¼ä¸ºæ­£ï¼‰
                my_advantage = get_situation_adv(
                    ego_state, enemy_state, center_lon, center_lat, center_alt
                )
                
                # è®¡ç®—æ•Œæ–¹å¯¹æˆ‘æ–¹çš„ä¼˜åŠ¿ï¼ˆå¯¹æ•Œæ–¹æœ‰åˆ©çš„å€¼ä¸ºæ­£ï¼‰
                enemy_advantage = get_situation_adv(
                    enemy_state, ego_state, center_lon, center_lat, center_alt
                )
                
                # è®¡ç®—ä¼˜åŠ¿å·®å€¼ï¼ˆæˆ‘æ–¹ä¼˜åŠ¿ - æ•Œæ–¹ä¼˜åŠ¿ï¼‰
                advantage_diff = my_advantage - enemy_advantage
                
                # æ·»åŠ è·ç¦»æƒ©ç½šå› å­ï¼Œä¼˜å…ˆé€‰æ‹©è·ç¦»è¾ƒè¿‘çš„ç›®æ ‡
                ego_cur_ned = LLA2NEU(*ego_state[:3], center_lon, center_lat, center_alt)
                enemy_cur_ned = LLA2NEU(*enemy_state[:3], center_lon, center_lat, center_alt)
                distance = np.linalg.norm([enemy_cur_ned[0] - ego_cur_ned[0], 
                                         enemy_cur_ned[1] - ego_cur_ned[1], 
                                         enemy_cur_ned[2] - ego_cur_ned[2]])
                
                # è·ç¦»æƒ©ç½šï¼šè·ç¦»è¶Šè¿œï¼Œä¼˜åŠ¿å€¼è¶Šä½
                distance_penalty = max(0, 1.0 - distance / 10000.0)  # 10å…¬é‡Œå†…æ— æƒ©ç½š
                adjusted_advantage_diff = advantage_diff * distance_penalty
                
                advantage_differences[enemy_id] = adjusted_advantage_diff
                advantage_details[enemy_id] = {
                    'my_advantage': my_advantage,
                    'enemy_advantage': enemy_advantage,
                    'advantage_diff': advantage_diff,
                    'adjusted_advantage_diff': adjusted_advantage_diff,
                    'distance': distance
                }
                
                logger.debug(f"{ego_agent_id} vs {enemy_id}: my_adv={my_advantage:.4f}, enemy_adv={enemy_advantage:.4f}, "
                           f"diff={advantage_diff:.4f}, adj_diff={adjusted_advantage_diff:.4f}, dist={distance:.1f}m")
                
            except Exception as e:
                logger.warning(f"Failed to calculate advantage for {ego_agent_id} vs {enemy_id}: {e}")
                advantage_differences[enemy_id] = -np.inf
        
        # é€‰æ‹©ä¼˜åŠ¿å·®å€¼æœ€å¤§çš„æ•Œæ–¹é£æœº
        if advantage_differences:
            best_enemy = max(advantage_differences.keys(), key=lambda x: advantage_differences[x])
            best_diff = advantage_differences[best_enemy]
            
            logger.info(f"{ego_agent_id} selected target: {best_enemy} "
                       f"(advantage_diff: {advantage_details[best_enemy]['advantage_diff']:.4f}, "
                       f"adjusted_diff: {best_diff:.4f}, "
                       f"distance: {advantage_details[best_enemy]['distance']:.1f}m)")
            return best_enemy, advantage_details[best_enemy]
        else:
            logger.warning(f"No valid targets found for {ego_agent_id}")
            return None, None
            
    except Exception as e:
        logger.error(f"Error in target selection for {ego_agent_id}: {e}")
        return None, None

def update_observation_with_target(obs, env, step_count, center_lon=120.0, center_lat=60.0, center_alt=0.0):
    """
    æ›´æ–°è§‚å¯Ÿæ•°æ®ï¼Œä¸ºæ¯æ¶æˆ‘æ–¹é£æœºé€‰æ‹©æœ€ä¼˜ç›®æ ‡å¹¶æ›´æ–°æ•Œæ–¹ä¿¡æ¯ - ç®€åŒ–ç‰ˆæœ¬
    æ¯æ­¥éƒ½é‡æ–°é€‰æ‹©æœ€ä¼˜ç›®æ ‡ï¼Œå…è®¸å¤šæ¶é£æœºé€‰æ‹©åŒä¸€ç›®æ ‡
    """
    try:
        # è·å–æˆ‘æ–¹é£æœºID
        friendly_ids = [agent_id for agent_id in env.agents.keys() if agent_id.startswith('A') and env.agents[agent_id].is_alive]
        
        # ä¸ºæ¯æ¶æˆ‘æ–¹é£æœºé€‰æ‹©æœ€ä¼˜ç›®æ ‡ï¼Œå…è®¸å¤šæ¶é£æœºé€‰æ‹©åŒä¸€ç›®æ ‡
        target_mapping = {}
        current_step_data = {
            'step': step_count,
            'timestamp': time.time(),
            'matches': []
        }
        
        # ä¸ºæ¯æ¶é£æœºç‹¬ç«‹é€‰æ‹©æœ€ä¼˜ç›®æ ‡
        for friendly_id in friendly_ids:
            target_id, advantage_info = select_best_target(friendly_id, env, center_lon, center_lat, center_alt)
            if target_id:
                target_mapping[friendly_id] = target_id
                
                # è®°å½•åŒ¹é…å’Œä¼˜åŠ¿ä¿¡æ¯
                match_data = {
                    'friendly_id': friendly_id,
                    'target_id': target_id,
                    'my_advantage': advantage_info['my_advantage'],
                    'enemy_advantage': advantage_info['enemy_advantage'],
                    'advantage_diff': advantage_info['advantage_diff'],
                    'adjusted_advantage_diff': advantage_info['adjusted_advantage_diff'],
                    'distance': advantage_info['distance']
                }
                current_step_data['matches'].append(match_data)
                
                logger.debug(f"{friendly_id} é€‰æ‹©ç›®æ ‡: {target_id} (ä¼˜åŠ¿å·®å€¼: {advantage_info['adjusted_advantage_diff']:.4f})")
        
        # æ·»åŠ åˆ°å…¨å±€å†å²è®°å½•
        if current_step_data['matches']:
            target_matching_history.append(current_step_data)
        
        # æ›´æ–°è§‚å¯Ÿæ•°æ®
        updated_obs = obs.copy()
        
        for i, agent_id in enumerate(env.agents.keys()):
            if agent_id.startswith('A') and agent_id in target_mapping:
                # è·å–ç›®æ ‡æ•Œæ–¹é£æœº
                target_id = target_mapping[agent_id]
                
                # è·å–æˆ‘æ–¹å’Œç›®æ ‡çš„çŠ¶æ€
                ego_agent = env.agents[agent_id]
                target_agent = env.agents[target_id]
                
                ego_state = np.array(ego_agent.get_property_values(env.task.state_var))
                target_state = np.array(target_agent.get_property_values(env.task.state_var))
                
                # è®¡ç®—ç›¸å¯¹ä¿¡æ¯ (6ç»´)
                ego_cur_ned = LLA2NEU(*ego_state[:3], center_lon, center_lat, center_alt)
                target_cur_ned = LLA2NEU(*target_state[:3], center_lon, center_lat, center_alt)
                
                ego_feature = np.array([*ego_cur_ned, *(ego_state[6:9])])
                target_feature = np.array([*target_cur_ned, *(target_state[6:9])])
                
                AO, TA, R, side_flag = get_AO_TA_R(ego_feature, target_feature, return_side=True)
                
                # æ„å»ºæ­£ç¡®çš„ç›¸å¯¹ä¿¡æ¯
                relative_info = np.array([
                    (target_state[9] - ego_state[9]) / 340,      # delta_v_body_x (unit: mh)
                    (target_state[2] - ego_state[2]) / 1000,    # delta_altitude (unit: km)
                    AO,                                          # ego_AO (unit: rad)
                    TA,                                          # ego_TA (unit: rad)
                    R / 10000,                                   # relative_distance (unit: 10km)
                    side_flag                                    # side_flag
                ])
                
                # æ›´æ–°æˆ‘æ–¹é£æœºçš„è§‚å¯Ÿæ•°æ®
                updated_obs[i] = np.concatenate([
                    obs[i][:9],  # ä¿æŒæˆ‘æ–¹ä¿¡æ¯ä¸å˜
                    relative_info  # ä½¿ç”¨æ­£ç¡®è®¡ç®—çš„ç›¸å¯¹ä¿¡æ¯
                ])
                
                logger.debug(f"Updated {agent_id} observation with target {target_id}")
        
        return updated_obs
        
    except Exception as e:
        logger.error(f"Error updating observations with targets: {e}")
        return obs

def plot_advantage_analysis():
    """ç»˜åˆ¶ä¼˜åŠ¿å‡½æ•°åˆ†æå›¾è¡¨"""
    if not target_matching_history:
        logger.warning("No target matching history to plot")
        return
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    # åˆ›å»ºå›¾è¡¨ - 2x2å¸ƒå±€ï¼Œæ¯ä¸ªå­å›¾ä»£è¡¨ä¸€æ¶æˆ‘æ–¹é£æœº
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('4v4 åˆ†å±‚1v1é£æ ¼ - å„é£æœºä¼˜åŠ¿å‡½æ•°å€¼å˜åŒ–', fontsize=16, fontweight='bold')
    
    # æˆ‘æ–¹é£æœºIDåˆ—è¡¨
    friendly_ids = ['A0100', 'A0200', 'A0300', 'A0400']
    
    # ä¸ºæ¯ä¸ªæˆ‘æ–¹é£æœºåˆ›å»ºå­å›¾
    for i, friendly_id in enumerate(friendly_ids):
        row = i // 2
        col = i % 2
        ax = axes[row, col]
        
        # æ”¶é›†è¯¥é£æœºçš„æ‰€æœ‰ä¼˜åŠ¿æ•°æ®
        my_advantages = []
        enemy_advantages = []
        steps = []
        
        # ä»å†å²è®°å½•ä¸­æå–è¯¥é£æœºçš„æ•°æ®
        for step_data in target_matching_history:
            for match in step_data['matches']:
                if match['friendly_id'] == friendly_id:
                    my_advantages.append(match['my_advantage'])
                    enemy_advantages.append(match['enemy_advantage'])
                    steps.append(step_data['step'])
                    break
        
        if steps:  # å¦‚æœæœ‰æ•°æ®æ‰ç»˜å›¾
            # ç»˜åˆ¶æˆ‘æ–¹ä¼˜åŠ¿å€¼ï¼ˆçº¢è‰²çº¿ï¼‰
            ax.plot(steps, my_advantages, color='red', linewidth=2, label='æˆ‘æ–¹ä¼˜åŠ¿å€¼', marker='o', markersize=3)
            
            # ç»˜åˆ¶æ•Œæ–¹ä¼˜åŠ¿å€¼ï¼ˆè“è‰²çº¿ï¼‰
            ax.plot(steps, enemy_advantages, color='blue', linewidth=2, label='æ•Œæ–¹ä¼˜åŠ¿å€¼', marker='s', markersize=3)
            
            ax.set_title(f'{friendly_id} ä¼˜åŠ¿å‡½æ•°å€¼å˜åŒ–')
            ax.set_xlabel('æ­¥æ•°')
            ax.set_ylabel('ä¼˜åŠ¿å‡½æ•°å€¼')
            ax.legend()
            ax.grid(True, alpha=0.3)
            
            # æ·»åŠ é›¶çº¿ä½œä¸ºå‚è€ƒ
            ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)
        else:
            ax.text(0.5, 0.5, f'{friendly_id}\næ— æ•°æ®', ha='center', va='center', transform=ax.transAxes)
            ax.set_title(f'{friendly_id}')
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    plot_filename = f"advantage_analysis_{timestamp}.png"
    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')
    logger.info(f"ä¼˜åŠ¿å‡½æ•°åˆ†æå›¾è¡¨å·²ä¿å­˜ä¸º: {plot_filename}")
    
    # æ˜¾ç¤ºå›¾è¡¨
    plt.show()
    
    # æ‰“å°ç®€è¦ç»Ÿè®¡ä¿¡æ¯
    print("\n=== ä¼˜åŠ¿å‡½æ•°åˆ†æç»Ÿè®¡ ===")
    print(f"æ€»æ­¥æ•°: {len(target_matching_history)}")
    
    for friendly_id in friendly_ids:
        my_advs = []
        enemy_advs = []
        for step_data in target_matching_history:
            for match in step_data['matches']:
                if match['friendly_id'] == friendly_id:
                    my_advs.append(match['my_advantage'])
                    enemy_advs.append(match['enemy_advantage'])
                    break
        
        if my_advs:
            print(f"{friendly_id} - æˆ‘æ–¹ä¼˜åŠ¿å€¼: å¹³å‡={np.mean(my_advs):.4f}, æ•Œæ–¹ä¼˜åŠ¿å€¼: å¹³å‡={np.mean(enemy_advs):.4f}")
def plot_target_assignment():
    """ç»˜åˆ¶ç›®æ ‡åˆ†é…æŸ±çŠ¶å›¾"""
    if not target_matching_history:
        logger.warning("No target matching history to plot")
        return
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    # åˆ›å»ºå›¾è¡¨ - 2x2å¸ƒå±€ï¼Œæ¯ä¸ªå­å›¾ä»£è¡¨ä¸€æ¶æˆ‘æ–¹é£æœº
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('4v4 åˆ†å±‚1v1é£æ ¼ - å„é£æœºç›®æ ‡åˆ†é…æƒ…å†µ', fontsize=16, fontweight='bold')
    
    # æˆ‘æ–¹é£æœºIDåˆ—è¡¨
    friendly_ids = ['A0100', 'A0200', 'A0300', 'A0400']
    
    # ä¸ºæ¯ä¸ªæˆ‘æ–¹é£æœºåˆ›å»ºå­å›¾
    for i, friendly_id in enumerate(friendly_ids):
        row = i // 2
        col = i % 2
        ax = axes[row, col]
        
        # æ”¶é›†è¯¥é£æœºçš„ç›®æ ‡åˆ†é…æ•°æ®
        steps = []
        target_ids = []
        
        # ä»å†å²è®°å½•ä¸­æå–è¯¥é£æœºçš„ç›®æ ‡åˆ†é…æ•°æ®
        for step_data in target_matching_history:
            for match in step_data['matches']:
                if match['friendly_id'] == friendly_id:
                    steps.append(step_data['step'])
                    target_ids.append(match['target_id'])
                    break
        
        if steps:  # å¦‚æœæœ‰æ•°æ®æ‰ç»˜å›¾
            # å°†ç›®æ ‡IDè½¬æ¢ä¸ºæ•°å­—ä¾¿äºç»˜å›¾
            target_mapping = {'B0100': 1, 'B0200': 2, 'B0300': 3, 'B0400': 4}
            target_numbers = [target_mapping.get(target_id, 0) for target_id in target_ids]
            
            # åˆ›å»ºæŸ±çŠ¶å›¾
            bars = ax.bar(steps, target_numbers, width=0.8, alpha=0.7, 
                         color=['red', 'blue', 'green', 'orange'][i], edgecolor='black', linewidth=0.5)
            
            ax.set_title(f'{friendly_id} ç›®æ ‡åˆ†é…æƒ…å†µ')
            ax.set_xlabel('æ­¥æ•°')
            ax.set_ylabel('ç›®æ ‡æ•Œæœºç¼–å·')
            ax.set_ylim(0.5, 4.5)
            ax.set_yticks([1, 2, 3, 4])
            ax.set_yticklabels(['B0100', 'B0200', 'B0300', 'B0400'])
            ax.grid(True, alpha=0.3, axis='y')
            
            # æ·»åŠ ç›®æ ‡åˆ†é…ç»Ÿè®¡ä¿¡æ¯
            target_counts = {}
            for target_id in target_ids:
                target_counts[target_id] = target_counts.get(target_id, 0) + 1
            
            # åœ¨å­å›¾å³ä¸Šè§’æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            stats_text = f"ç›®æ ‡åˆ†é…ç»Ÿè®¡:\n"
            for target_id, count in sorted(target_counts.items()):
                percentage = (count / len(target_ids)) * 100
                stats_text += f"{target_id}: {count}æ¬¡ ({percentage:.1f}%)\n"
            
            ax.text(0.98, 0.98, stats_text.strip(), transform=ax.transAxes, 
                   verticalalignment='top', horizontalalignment='right',
                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),
                   fontsize=8)
            
        else:
            ax.text(0.5, 0.5, f'{friendly_id}\næ— æ•°æ®', ha='center', va='center', transform=ax.transAxes)
            ax.set_title(f'{friendly_id}')
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    plot_filename = f"target_assignment_{timestamp}.png"
    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')
    logger.info(f"ç›®æ ‡åˆ†é…æŸ±çŠ¶å›¾å·²ä¿å­˜ä¸º: {plot_filename}")
    
    # æ˜¾ç¤ºå›¾è¡¨
    plt.show()
    
    # æ‰“å°ç›®æ ‡åˆ†é…ç»Ÿè®¡ä¿¡æ¯
    print("\n=== ç›®æ ‡åˆ†é…ç»Ÿè®¡ ===")
    print(f"æ€»æ­¥æ•°: {len(target_matching_history)}")
    
    for friendly_id in friendly_ids:
        target_counts = {}
        for step_data in target_matching_history:
            for match in step_data['matches']:
                if match['friendly_id'] == friendly_id:
                    target_id = match['target_id']
                    target_counts[target_id] = target_counts.get(target_id, 0) + 1
                    break
        
        if target_counts:
            print(f"{friendly_id} ç›®æ ‡åˆ†é…:")
            for target_id, count in sorted(target_counts.items()):
                percentage = (count / sum(target_counts.values())) * 100
                print(f"  {target_id}: {count}æ¬¡ ({percentage:.1f}%)")
        else:
            print(f"{friendly_id}: æ— ç›®æ ‡åˆ†é…æ•°æ®")
def main():
    # 4v4 åˆ†å±‚1v1é£æ ¼é…ç½®
    num_agents = 8  # 4æ¶çº¢æ–¹ + 4æ¶è“æ–¹
    render = True
    ego_policy_index = "1040"  # ä½¿ç”¨latestæ¨¡å‹
    enm_policy_index = "250"
    
    # åˆ†å±‚1v1æ¨¡å‹è·¯å¾„ - ä½¿ç”¨æ‚¨æä¾›çš„è·¯å¾„
    ego_run_dir = "scripts/results/SingleCombat/1v1/NoWeapon/HierarchySelfplay/ppo/v1/wandb/latest-run/files"
    enm_run_dir = ego_run_dir  # ä½¿ç”¨åŒä¸€ä¸ªæ¨¡å‹ä½œä¸ºåŒæ–¹
    
    # å¦‚æœè·¯å¾„ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„
    if not os.path.exists(ego_run_dir):
        logger.warning(f"Model path not found: {ego_run_dir}")
        ego_run_dir = "results/SingleCombat/1v1/NoWeapon/HierarchySelfplay/ppo/v1/wandb/latest-run/files"
    
    experiment_name = "4v4_hierarchy_1v1_blood"
    
    # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    experiment_name_with_timestamp = f"{experiment_name}_{timestamp}"
    
    # åˆ›å»º4v4 åˆ†å±‚1v1é£æ ¼ç¯å¢ƒ
    logger.info("Creating 4v4 hierarchical 1v1 style environment...")
    try:
        env = MultipleCombatEnv("4v4/NoWeapon/Hierarchy1v1Style")
        env.seed(0)
        logger.info(f"Environment created successfully. Num agents: {env.num_agents}")
        logger.info(f"Observation space: {env.observation_space}")
        logger.info(f"Action space: {env.action_space}")
        
        # éªŒè¯è§‚å¯Ÿç©ºé—´ç»´åº¦
        expected_obs_length = 15  # 9 + 6
        actual_obs_length = env.observation_space.shape[0]
        logger.info(f"Expected observation length: {expected_obs_length}")
        logger.info(f"Actual observation length: {actual_obs_length}")
        
        if actual_obs_length != expected_obs_length:
            logger.error(f"Observation space mismatch! Expected {expected_obs_length}, got {actual_obs_length}")
            return
            
    except Exception as e:
        logger.error(f"Failed to create environment: {e}")
        return
    
    args = Args()
    
    # åˆ›å»ºç­–ç•¥ç½‘ç»œ
    logger.info("Creating policy networks...")
    try:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"Using device: {device}")
        
        ego_policy = PPOActor(args, env.observation_space, env.action_space, device=device)
        enm_policy = PPOActor(args, env.observation_space, env.action_space, device=device)
        ego_policy.eval()
        enm_policy.eval()
        
        # åŠ è½½æ¨¡å‹æƒé‡
        ego_model_path = os.path.join(ego_run_dir, f"actor_{ego_policy_index}.pt")
        enm_model_path = os.path.join(enm_run_dir, f"actor_{enm_policy_index}.pt")
        
        ego_loaded = load_model_safely(ego_model_path, ego_policy, device)
        enm_loaded = load_model_safely(enm_model_path, enm_policy, device)
        
        if not ego_loaded or not enm_loaded:
            logger.warning("Using random initialized policies")
            
    except Exception as e:
        logger.error(f"Failed to create policy networks: {e}")
        return
    
    logger.info("Starting 4v4 hierarchical 1v1 style render...")
    obs, _ = env.reset()
    
    # åˆå§‹ç›®æ ‡é€‰æ‹©
    obs = update_observation_with_target(obs, env, 0)
    
    if render:
        render_file = f'{experiment_name_with_timestamp}.txt.acmi'
        env.render(mode='txt', filepath=render_file)
        logger.info(f"Rendering to: {render_file}")
    
    # RNNçŠ¶æ€åˆå§‹åŒ– - ä¿®å¤ç»´åº¦é—®é¢˜
    ego_rnn_states = np.zeros((num_agents // 2, 1, 128), dtype=np.float32)  # (4, 1, 128)
    enm_rnn_states = np.zeros_like(ego_rnn_states, dtype=np.float32)  # (4, 1, 128)
    masks = np.ones((num_agents // 2, 1))  # 4æ¶é£æœºçš„æ©ç 
    
    # è§‚å¯Ÿæ•°æ®åˆ‡ç‰‡ï¼šå‰4æ¶ä¸ºçº¢æ–¹ï¼Œå4æ¶ä¸ºè“æ–¹
    enm_obs = obs[num_agents // 2:, :]  # è“æ–¹è§‚å¯Ÿ (4, 15)
    ego_obs = obs[:num_agents // 2, :]  # çº¢æ–¹è§‚å¯Ÿ (4, 15)
    
    episode_rewards = np.zeros((num_agents // 2, 1))
    step_count = 0
    
    try:
        while True:
            step_count += 1
            start = time.time()
            
            # çº¢æ–¹ç­–ç•¥ç½‘ç»œæ¨ç†
            ego_actions, _, ego_rnn_states = ego_policy(ego_obs, ego_rnn_states, masks, deterministic=True)
            ego_actions = _t2n(ego_actions)
            ego_rnn_states = _t2n(ego_rnn_states)
            
            # è“æ–¹ç­–ç•¥ç½‘ç»œæ¨ç†
            enm_actions, _, enm_rnn_states = enm_policy(enm_obs, enm_rnn_states, masks, deterministic=True)
            enm_actions = _t2n(enm_actions)
            enm_rnn_states = _t2n(enm_rnn_states)
            
            # åˆå¹¶åŠ¨ä½œ
            actions = np.concatenate((ego_actions, enm_actions), axis=0)
            
            # ç¯å¢ƒæ­¥è¿›
            obs, _, rewards, dones, infos = env.step(actions)
            
            # åº”ç”¨ä¼¤å®³ç³»ç»Ÿ
            damage_log = apply_damage_system(env)
            
            # å¦‚æœæœ‰ä¼¤å®³å‘ç”Ÿï¼Œæ‰“å°è¯¦ç»†ä¿¡æ¯
            if damage_log:
                logger.info(f"Step {step_count} - ä¼¤å®³äº‹ä»¶:")
                for damage_event in damage_log:
                    logger.info(f"  {damage_event['attacker']} -> {damage_event['target']}: "
                              f"ä¼¤å®³={damage_event['damage']:.4f}, "
                              f"è·ç¦»={damage_event['distance']:.1f}m, "
                              f"ç±»å‹={damage_event['type']}")
            
            # æ›´æ–°ç›®æ ‡é€‰æ‹©
            obs = update_observation_with_target(obs, env, step_count)
            
            # è®¡ç®—çº¢æ–¹æ€»å¥–åŠ±
            red_rewards = rewards[:num_agents // 2, ...]
            episode_rewards += red_rewards
            
            if render:
                env.render(mode='txt', filepath=render_file)
            
            if dones.all():
                logger.info(f"Episode finished at step {step_count}")
                logger.info(f"Episode info: {infos}")
                break
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰é£æœºéƒ½è¢«å‡»è½
            alive_agents = sum(1 for agent in env.agents.values() if agent.is_alive)
            if alive_agents == 0:
                logger.info(f"All aircraft destroyed at step {step_count}")
                break
            
            # æ‰“å°è¡€é‡ä¿¡æ¯
            bloods = [env.agents[agent_id].bloods for agent_id in env.agents.keys()]
            if step_count % 50 == 0:  # æ¯50æ­¥æ‰“å°ä¸€æ¬¡
                logger.info(f"Step: {step_count}")
                logger.info("é£æœºçŠ¶æ€:")
                for agent_id in env.agents.keys():
                    agent = env.agents[agent_id]
                    status = "å­˜æ´»" if agent.is_alive else "å‡»è½"
                    logger.info(f"  {agent_id}: è¡€é‡={agent.bloods:.2f}, çŠ¶æ€={status}")
                logger.info(f"Red team rewards: {red_rewards.flatten()}")
                
                # æ‰“å°å¯¹åº”å…³ç³»
                logger.info("Corresponding pairs:")
                for i in range(4):
                    red_id = f"A0{i+1}00"
                    blue_id = f"B0{i+1}00"
                    logger.info(f"  {red_id} <-> {blue_id}")
            
            # æ›´æ–°è§‚å¯Ÿæ•°æ®
            enm_obs = obs[num_agents // 2:, ...]
            ego_obs = obs[:num_agents // 2, ...]
            
    except KeyboardInterrupt:
        logger.info("Render interrupted by user")
    except Exception as e:
        logger.error(f"Error during render: {e}")
    
    # è¾“å‡ºæœ€ç»ˆç»“æœ
    logger.info(f"Final episode rewards: {episode_rewards.flatten()}")
    logger.info(f"Average episode reward: {np.mean(episode_rewards):.4f}")
    logger.info(f"Total steps: {step_count}")
    
    # è¾“å‡ºæœ€ç»ˆé£æœºçŠ¶æ€
    logger.info("Final aircraft status:")
    red_survivors = 0
    blue_survivors = 0
    for agent_id, agent in env.agents.items():
        status = "å­˜æ´»" if agent.is_alive else "å‡»è½"
        team = "çº¢æ–¹" if agent_id.startswith('A') else "è“æ–¹"
        if agent.is_alive:
            if agent_id.startswith('A'):
                red_survivors += 1
            else:
                blue_survivors += 1
        logger.info(f"  {agent_id} ({team}): è¡€é‡={agent.bloods:.2f}, çŠ¶æ€={status}")
    
    logger.info(f"çº¢æ–¹å¹¸å­˜è€…: {red_survivors}/4, è“æ–¹å¹¸å­˜è€…: {blue_survivors}/4")
    if red_survivors > blue_survivors:
        logger.info("çº¢æ–¹è·èƒœ!")
    elif blue_survivors > red_survivors:
        logger.info("è“æ–¹è·èƒœ!")
    else:
        logger.info("å¹³å±€!")
    
    # éªŒè¯å¯¹åº”å…³ç³»
    logger.info("Verifying corresponding enemy relationships:")
    for agent_id in env.agents.keys():
        if agent_id.startswith('A'):
            corresponding_enemy = 'B' + agent_id[1:]
            logger.info(f"  {agent_id} -> {corresponding_enemy}")
        elif agent_id.startswith('B'):
            corresponding_enemy = 'A' + agent_id[1:]
            logger.info(f"  {agent_id} -> {corresponding_enemy}")
    
    # ç»˜åˆ¶ä¼˜åŠ¿å‡½æ•°åˆ†æå›¾è¡¨
    logger.info("Generating advantage function analysis plots...")
    plot_advantage_analysis()
    logger.info("Generating target assignment plots...")
    plot_target_assignment()
if __name__ == "__main__":
    main() 