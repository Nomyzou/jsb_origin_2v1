# ğŸ” ShareJSBSimRunner ä»£ç åŠŸèƒ½æ·±åº¦è§£æ

## ğŸ“‹ **æ•´ä½“åŠŸèƒ½æ¦‚è¿°**

`ShareJSBSimRunner` æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºå¤šæ™ºèƒ½ä½“ç¯å¢ƒè®¾è®¡çš„è®­ç»ƒå™¨ï¼Œç»§æ‰¿è‡ªåŸºç¡€ `Runner` ç±»ã€‚å®ƒä¸»è¦è´Ÿè´£ï¼š

1. **å¤šæ™ºèƒ½ä½“è®­ç»ƒç®¡ç†**: åè°ƒå¤šä¸ªæ™ºèƒ½ä½“çš„è®­ç»ƒè¿‡ç¨‹
2. **å…±äº«è§‚å¯Ÿæ”¯æŒ**: å¤„ç†æ™ºèƒ½ä½“é—´çš„ä¿¡æ¯å…±äº«
3. **è‡ªå¯¹å¼ˆè®­ç»ƒ**: å®ç°FSP (Fictitious Self-Play) ç®—æ³•
4. **MAPPOç®—æ³•æ‰§è¡Œ**: è¿è¡Œå¤šæ™ºèƒ½ä½“PPOè®­ç»ƒ
5. **ç¯å¢ƒäº¤äº’ç®¡ç†**: ç®¡ç†è®­ç»ƒç¯å¢ƒå’Œè¯„ä¼°ç¯å¢ƒ

## ğŸ—ï¸ **ç±»ç»“æ„å’Œæ–¹æ³•åˆ†æ**

### **1. åˆå§‹åŒ–æ–¹æ³• `load()`**

```python
def load(self):
    # è·å–ç¯å¢ƒç©ºé—´ä¿¡æ¯
    self.obs_space = self.envs.observation_space          # ä¸ªä½“è§‚å¯Ÿç©ºé—´
    self.share_obs_space = self.envs.share_observation_space  # å…±äº«è§‚å¯Ÿç©ºé—´
    self.act_space = self.envs.action_space               # åŠ¨ä½œç©ºé—´
    self.num_agents = self.envs.num_agents                # æ™ºèƒ½ä½“æ•°é‡
    self.use_selfplay = self.all_args.use_selfplay        # æ˜¯å¦ä½¿ç”¨è‡ªå¯¹å¼ˆ
```

**åŠŸèƒ½è§£æ:**
- **ç¯å¢ƒç©ºé—´è·å–**: ä»ç¯å¢ƒä¸­æå–è§‚å¯Ÿã€å…±äº«è§‚å¯Ÿã€åŠ¨ä½œç©ºé—´çš„å®šä¹‰
- **æ™ºèƒ½ä½“æ•°é‡**: ç¡®å®šå‚ä¸è®­ç»ƒçš„æ™ºèƒ½ä½“æ€»æ•°
- **è‡ªå¯¹å¼ˆæ ‡å¿—**: åˆ¤æ–­æ˜¯å¦å¯ç”¨è‡ªå¯¹å¼ˆè®­ç»ƒæ¨¡å¼

```python
# ç®—æ³•ç»„ä»¶åˆ›å»º
if self.algorithm_name == "mappo":
    from algorithms.mappo.ppo_trainer import PPOTrainer as Trainer
    from algorithms.mappo.ppo_policy import PPOPolicy as Policy
self.policy = Policy(self.all_args, self.obs_space, self.share_obs_space, self.act_space, device=self.device)
self.trainer = Trainer(self.all_args, device=self.device)
```

**åŠŸèƒ½è§£æ:**
- **ç®—æ³•é€‰æ‹©**: ç›®å‰åªæ”¯æŒMAPPOç®—æ³•
- **ç­–ç•¥ç½‘ç»œ**: åˆ›å»ºåŒ…å«Actorå’ŒCriticçš„ç­–ç•¥ç½‘ç»œ
- **è®­ç»ƒå™¨**: åˆ›å»ºPPOè®­ç»ƒå™¨ï¼Œè´Ÿè´£ç­–ç•¥æ›´æ–°

```python
# ç»éªŒå›æ”¾ç¼“å†²åŒºåˆ›å»º
if self.use_selfplay:
    self.buffer = SharedReplayBuffer(self.all_args, self.num_agents // 2, self.obs_space, self.share_obs_space, self.act_space)
else:
    self.buffer = SharedReplayBuffer(self.all_args, self.num_agents, self.obs_space, self.share_obs_space, self.act_space)
```

**åŠŸèƒ½è§£æ:**
- **è‡ªå¯¹å¼ˆæ¨¡å¼**: åªå­˜å‚¨æˆ‘æ–¹æ™ºèƒ½ä½“çš„ç»éªŒ (num_agents // 2)
- **æ­£å¸¸æ¨¡å¼**: å­˜å‚¨æ‰€æœ‰æ™ºèƒ½ä½“çš„ç»éªŒ
- **å…±äº«ç¼“å†²åŒº**: æ”¯æŒæ™ºèƒ½ä½“é—´çš„ç»éªŒå…±äº«

### **2. è‡ªå¯¹å¼ˆç»„ä»¶åˆå§‹åŒ–**

```python
if self.use_selfplay:
    from algorithms.utils.selfplay import get_algorithm
    self.selfplay_algo = get_algorithm(self.all_args.selfplay_algorithm)
    
    # å¯¹æ‰‹ç­–ç•¥æ± ç®¡ç†
    self.policy_pool = {'latest': self.all_args.init_elo}  # ELOè¯„åˆ†ç³»ç»Ÿ
    self.opponent_policy = [
        Policy(self.all_args, self.obs_space, self.share_obs_space, self.act_space, device=self.device)
        for _ in range(self.all_args.n_choose_opponents)]
    
    # ç¯å¢ƒåˆ†é…ç­–ç•¥
    self.opponent_env_split = np.array_split(np.arange(self.n_rollout_threads), len(self.opponent_policy))
    
    # å¯¹æ‰‹æ•°æ®å­˜å‚¨
    self.opponent_obs = np.zeros_like(self.buffer.obs[0])
    self.opponent_rnn_states = np.zeros_like(self.buffer.rnn_states_actor[0])
    self.opponent_masks = np.ones_like(self.buffer.masks[0])
```

**åŠŸèƒ½è§£æ:**
- **è‡ªå¯¹å¼ˆç®—æ³•**: æ”¯æŒFSPç­‰è‡ªå¯¹å¼ˆç®—æ³•
- **ç­–ç•¥æ± **: ç®¡ç†å†å²ç­–ç•¥ç‰ˆæœ¬å’ŒELOè¯„åˆ†
- **å¯¹æ‰‹ç­–ç•¥**: åˆ›å»ºå¤šä¸ªå¯¹æ‰‹ç­–ç•¥å®ä¾‹
- **ç¯å¢ƒåˆ†é…**: å°†å¹¶è¡Œç¯å¢ƒåˆ†é…ç»™ä¸åŒå¯¹æ‰‹ç­–ç•¥
- **æ•°æ®å­˜å‚¨**: ä¸ºå¯¹æ‰‹ç­–ç•¥åˆ†é…ç‹¬ç«‹çš„å­˜å‚¨ç©ºé—´

### **3. ä¸»è®­ç»ƒå¾ªç¯ `run()`**

```python
def run(self):
    self.warmup()  # é¢„çƒ­é˜¶æ®µ
    
    start = time.time()
    self.total_num_steps = 0
    episodes = self.num_env_steps // self.buffer_size // self.n_rollout_threads
    
    for episode in range(episodes):
        # æ•°æ®æ”¶é›†é˜¶æ®µ
        for step in range(self.buffer_size):
            values, actions, action_log_probs, rnn_states_actor, rnn_states_critic = self.collect(step)
            obs, share_obs, rewards, dones, infos = self.envs.step(actions)
            data = obs, share_obs, actions, rewards, dones, action_log_probs, values, rnn_states_actor, rnn_states_critic
            self.insert(data)
        
        # è®­ç»ƒé˜¶æ®µ
        self.compute()           # è®¡ç®—å›æŠ¥å’Œä¼˜åŠ¿
        train_infos = self.train()  # è®­ç»ƒç­–ç•¥ç½‘ç»œ
        
        # åå¤„ç†
        self.total_num_steps = (episode + 1) * self.buffer_size * self.n_rollout_threads
        
        # æ¨¡å‹ä¿å­˜
        if (episode % self.save_interval == 0) or (episode == episodes - 1):
            self.save(episode)
        
        # æ—¥å¿—è®°å½•
        if episode % self.log_interval == 0:
            self.log_info(train_infos, self.total_num_steps)
        
        # è¯„ä¼°
        if episode % self.eval_interval == 0 and self.use_eval:
            self.eval(self.total_num_steps)
```

**åŠŸèƒ½è§£æ:**
- **é¢„çƒ­é˜¶æ®µ**: åˆå§‹åŒ–ç¯å¢ƒå’Œç¼“å†²åŒº
- **æ•°æ®æ”¶é›†**: æ”¶é›†ä¸€ä¸ªå®Œæ•´ç¼“å†²åŒºçš„è®­ç»ƒæ•°æ®
- **ç­–ç•¥è®­ç»ƒ**: ä½¿ç”¨æ”¶é›†çš„æ•°æ®è®­ç»ƒç­–ç•¥ç½‘ç»œ
- **å®šæœŸä¿å­˜**: æŒ‰é—´éš”ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
- **æ€§èƒ½è¯„ä¼°**: å®šæœŸè¯„ä¼°å½“å‰ç­–ç•¥æ€§èƒ½

### **4. æ•°æ®æ”¶é›† `collect()`**

```python
@torch.no_grad()
def collect(self, step):
    self.policy.prep_rollout()  # å‡†å¤‡æ¨ç†æ¨¡å¼
    
    # è·å–æˆ‘æ–¹æ™ºèƒ½ä½“åŠ¨ä½œ
    values, actions, action_log_probs, rnn_states_actor, rnn_states_critic \
        = self.policy.get_actions(np.concatenate(self.buffer.share_obs[step]),
                                  np.concatenate(self.buffer.obs[step]),
                                  np.concatenate(self.buffer.rnn_states_actor[step]),
                                  np.concatenate(self.buffer.rnn_states_critic[step]),
                                  np.concatenate(self.buffer.masks[step]))
    
    # åˆ†å‰²å¹¶è¡Œæ•°æ® [N*M, shape] => [N, M, shape]
    values = np.array(np.split(_t2n(values), self.n_rollout_threads))
    actions = np.array(np.split(_t2n(actions), self.n_rollout_threads))
    # ... å…¶ä»–æ•°æ®åˆ†å‰²
    
    # è‡ªå¯¹å¼ˆ: è·å–å¯¹æ‰‹ç­–ç•¥åŠ¨ä½œ
    if self.use_selfplay:
        opponent_actions = np.zeros_like(actions)
        for policy_idx, policy in enumerate(self.opponent_policy):
            env_idx = self.opponent_env_split[policy_idx]
            opponent_action, opponent_rnn_states \
                = policy.act(np.concatenate(self.opponent_obs[env_idx]),
                             np.concatenate(self.opponent_rnn_states[env_idx]),
                             np.concatenate(self.opponent_masks[env_idx]))
            opponent_actions[env_idx] = np.array(np.split(_t2n(opponent_action), len(env_idx)))
            self.opponent_rnn_states[env_idx] = np.array(np.split(_t2n(opponent_rnn_states), len(env_idx)))
        actions = np.concatenate((actions, opponent_actions), axis=1)
    
    return values, actions, action_log_probs, rnn_states_actor, rnn_states_critic
```

**åŠŸèƒ½è§£æ:**
- **ç­–ç•¥æ¨ç†**: ä½¿ç”¨å½“å‰ç­–ç•¥ç”ŸæˆåŠ¨ä½œ
- **æ•°æ®åˆ†å‰²**: å°†æ‰¹é‡æ•°æ®åˆ†å‰²ä¸ºå¹¶è¡Œç¯å¢ƒæ ¼å¼
- **å¯¹æ‰‹åŠ¨ä½œ**: åœ¨è‡ªå¯¹å¼ˆæ¨¡å¼ä¸‹ï¼Œä½¿ç”¨å¯¹æ‰‹ç­–ç•¥ç”ŸæˆåŠ¨ä½œ
- **ç¯å¢ƒåˆ†é…**: ä¸åŒç¯å¢ƒä½¿ç”¨ä¸åŒçš„å¯¹æ‰‹ç­–ç•¥

### **5. æ•°æ®æ’å…¥ `insert()`**

```python
def insert(self, data: List[np.ndarray]):
    obs, share_obs, actions, rewards, dones, action_log_probs, values, rnn_states_actor, rnn_states_critic = data
    
    # å¤„ç†ç»ˆæ­¢çŠ¶æ€
    dones = dones.squeeze(axis=-1)
    dones_env = np.all(dones, axis=-1)
    
    # é‡ç½®ç»ˆæ­¢ç¯å¢ƒçš„RNNçŠ¶æ€
    rnn_states_actor[dones_env == True] = np.zeros(((dones_env == True).sum(), *rnn_states_actor.shape[1:]), dtype=np.float32)
    rnn_states_critic[dones_env == True] = np.zeros(((dones_env == True).sum(), *rnn_states_critic.shape[1:]), dtype=np.float32)
    
    # åˆ›å»ºæ©ç 
    masks = np.ones((self.n_rollout_threads, self.num_agents, 1), dtype=np.float32)
    masks[dones_env == True] = np.zeros(((dones_env == True).sum(), self.num_agents, 1), dtype=np.float32)
    
    active_masks = np.ones((self.n_rollout_threads, self.num_agents, 1), dtype=np.float32)
    active_masks[dones == True] = np.zeros(((dones == True).sum(), 1), dtype=np.float32)
    active_masks[dones_env == True] = np.ones(((dones_env == True).sum(), self.num_agents, 1), dtype=np.float32)
    
    # è‡ªå¯¹å¼ˆ: åˆ†ç¦»æˆ‘æ–¹å’Œå¯¹æ‰‹æ•°æ®
    if self.use_selfplay:
        self.opponent_obs = obs[:, self.num_agents // 2:, ...]
        self.opponent_masks = masks[:, self.num_agents // 2:, ...]
        
        obs = obs[:, :self.num_agents // 2, ...]
        share_obs = share_obs[:, :self.num_agents // 2, ...]
        actions = actions[:, :self.num_agents // 2, ...]
        rewards = rewards[:, :self.num_agents // 2, ...]
        masks = masks[:, :self.num_agents // 2, ...]
        active_masks = active_masks[:, :self.num_agents // 2, ...]
    
    # æ’å…¥ç¼“å†²åŒº
    self.buffer.insert(obs, share_obs, actions, rewards, masks, action_log_probs, values, \
        rnn_states_actor, rnn_states_critic, active_masks = active_masks)
```

**åŠŸèƒ½è§£æ:**
- **ç»ˆæ­¢å¤„ç†**: å¤„ç†episodeç»“æŸæ—¶çš„çŠ¶æ€é‡ç½®
- **RNNé‡ç½®**: ç»ˆæ­¢ç¯å¢ƒé‡ç½®RNNéšè—çŠ¶æ€
- **æ©ç åˆ›å»º**: åˆ›å»ºç”¨äºè®­ç»ƒçš„æ©ç 
- **æ•°æ®åˆ†ç¦»**: è‡ªå¯¹å¼ˆæ¨¡å¼ä¸‹åˆ†ç¦»æˆ‘æ–¹å’Œå¯¹æ‰‹æ•°æ®
- **ç¼“å†²åŒºæ’å…¥**: å°†å¤„ç†åçš„æ•°æ®æ’å…¥ç»éªŒå›æ”¾ç¼“å†²åŒº

### **6. è¯„ä¼°æ–¹æ³• `eval()`**

```python
@torch.no_grad()
def eval(self, total_num_steps):
    logging.info("\nStart evaluation...")
    total_episodes, eval_episode_rewards = 0, []
    eval_cumulative_rewards = np.zeros((self.n_eval_rollout_threads, *self.buffer.rewards.shape[2:]), dtype=np.float32)
    
    # ç¯å¢ƒé‡ç½®
    eval_obs, eval_share_obs = self.eval_envs.reset()
    eval_masks = np.ones((self.n_eval_rollout_threads, *self.buffer.masks.shape[2:]), dtype=np.float32)
    eval_rnn_states = np.zeros((self.n_eval_rollout_threads, *self.buffer.rnn_states_actor.shape[2:]), dtype=np.float32)
    
    # è‡ªå¯¹å¼ˆ: é€‰æ‹©è¯„ä¼°å¯¹æ‰‹
    if self.use_selfplay:
        eval_choose_opponents = [self.selfplay_algo.choose(self.policy_pool) for _ in range(self.all_args.n_choose_opponents)]
        eval_each_episodes = self.eval_episodes // self.all_args.n_choose_opponents
        eval_cur_opponent_idx = 0
    
    # è¯„ä¼°å¾ªç¯
    while total_episodes < self.eval_episodes:
        # åŠ è½½å¯¹æ‰‹ç­–ç•¥
        if self.use_selfplay and total_episodes >= eval_cur_opponent_idx * eval_each_episodes:
            policy_idx = eval_choose_opponents[eval_cur_opponent_idx]
            self.eval_opponent_policy.actor.load_state_dict(torch.load(str(self.save_dir) + f'/actor_{policy_idx}.pt', weights_only=True))
            eval_cur_opponent_idx += 1
        
        # è·å–åŠ¨ä½œå¹¶æ‰§è¡Œ
        eval_actions, eval_rnn_states = self.policy.act(...)
        if self.use_selfplay:
            eval_opponent_actions, eval_opponent_rnn_states = self.eval_opponent_policy.act(...)
            eval_actions = np.concatenate((eval_actions, eval_opponent_actions), axis=1)
        
        # ç¯å¢ƒæ­¥è¿›
        eval_obs, eval_share_obs, eval_rewards, eval_dones, eval_infos = self.eval_envs.step(eval_actions)
        
        # å¤„ç†å¥–åŠ±å’Œç»ˆæ­¢
        if self.use_selfplay:
            eval_rewards = eval_rewards[:, :self.num_agents // 2, ...]
        
        eval_cumulative_rewards += eval_rewards
        eval_dones_env = np.all(eval_dones.squeeze(axis=-1), axis=-1)
        total_episodes += np.sum(eval_dones_env)
        
        # é‡ç½®ç»ˆæ­¢ç¯å¢ƒ
        eval_masks[eval_dones_env == True] = np.zeros(...)
        eval_rnn_states[eval_dones_env == True] = np.zeros(...)
    
    # è®¡ç®—è¯„ä¼°ç»“æœ
    eval_infos = {}
    eval_infos['eval_average_episode_rewards'] = np.concatenate(eval_episode_rewards).mean()
    self.log_info(eval_infos, total_num_steps)
```

**åŠŸèƒ½è§£æ:**
- **è¯„ä¼°åˆå§‹åŒ–**: è®¾ç½®è¯„ä¼°ç¯å¢ƒå’Œåˆå§‹çŠ¶æ€
- **å¯¹æ‰‹é€‰æ‹©**: ä»ç­–ç•¥æ± ä¸­é€‰æ‹©è¯„ä¼°å¯¹æ‰‹
- **ç­–ç•¥æ‰§è¡Œ**: ä½¿ç”¨å½“å‰ç­–ç•¥å’Œå¯¹æ‰‹ç­–ç•¥è¿›è¡Œå¯¹æŠ—
- **æ€§èƒ½ç»Ÿè®¡**: æ”¶é›†å’Œè®¡ç®—è¯„ä¼°æŒ‡æ ‡
- **ç»“æœè®°å½•**: è®°å½•è¯„ä¼°ç»“æœåˆ°æ—¥å¿—ç³»ç»Ÿ

### **7. æ¸²æŸ“æ–¹æ³• `render()`**

```python
@torch.no_grad()
def render(self):
    logging.info("\nStart render ...")
    self.render_opponent_index = self.all_args.render_opponent_index
    render_episode_rewards = 0
    
    # ç¯å¢ƒé‡ç½®
    render_obs, render_share_obs = self.envs.reset()
    render_masks = np.ones((1, *self.buffer.masks.shape[2:]), dtype=np.float32)
    render_rnn_states = np.zeros((1, *self.buffer.rnn_states_actor.shape[2:]), dtype=np.float32)
    
    # å¼€å§‹æ¸²æŸ“
    self.envs.render(mode='txt', filepath=f'{self.run_dir}/{self.experiment_name}.txt.acmi')
    
    # è‡ªå¯¹å¼ˆ: åŠ è½½æŒ‡å®šå¯¹æ‰‹ç­–ç•¥
    if self.use_selfplay:
        policy_idx = self.render_opponent_index
        self.eval_opponent_policy.actor.load_state_dict(torch.load(str(self.model_dir) + f'/actor_{policy_idx}.pt', weights_only=True))
        # ... è®¾ç½®å¯¹æ‰‹çŠ¶æ€
    
    # æ¸²æŸ“å¾ªç¯
    while True:
        # è·å–åŠ¨ä½œ
        render_actions, render_rnn_states = self.policy.act(...)
        if self.use_selfplay:
            render_opponent_actions, render_opponent_rnn_states = self.eval_opponent_policy.act(...)
            render_actions = np.concatenate((render_actions, render_opponent_actions), axis=1)
        
        # ç¯å¢ƒæ­¥è¿›å’Œæ¸²æŸ“
        render_obs, render_share_obs, render_rewards, render_dones, render_infos = self.envs.step(render_actions)
        self.envs.render(mode='txt', filepath=f'{self.run_dir}/{self.experiment_name}.txt.acmi')
        
        if render_dones.all():
            break
```

**åŠŸèƒ½è§£æ:**
- **æ¸²æŸ“åˆå§‹åŒ–**: è®¾ç½®æ¸²æŸ“ç¯å¢ƒå’ŒçŠ¶æ€
- **å¯¹æ‰‹åŠ è½½**: åŠ è½½æŒ‡å®šçš„å¯¹æ‰‹ç­–ç•¥è¿›è¡Œå¯¹æŠ—
- **å®æ—¶æ¸²æŸ“**: å°†è®­ç»ƒè¿‡ç¨‹ä¿å­˜ä¸ºTacviewæ ¼å¼æ–‡ä»¶
- **å¾ªç¯æ§åˆ¶**: æ§åˆ¶æ¸²æŸ“ç›´åˆ°episodeç»“æŸ

### **8. æ¨¡å‹ä¿å­˜ `save()`**

```python
def save(self, episode):
    # ä¿å­˜æœ€æ–°æ¨¡å‹
    policy_actor_state_dict = self.policy.actor.state_dict()
    torch.save(policy_actor_state_dict, str(self.save_dir) + '/actor_latest.pt')
    policy_critic_state_dict = self.policy.critic.state_dict()
    torch.save(policy_critic_state_dict, str(self.save_dir) + '/critic_latest.pt')
    
    # è‡ªå¯¹å¼ˆ: ä¿å­˜ç­–ç•¥ç‰ˆæœ¬å’Œæ€§èƒ½
    if self.use_selfplay:
        torch.save(policy_actor_state_dict, str(self.save_dir) + f'/actor_{episode}.pt')
        self.policy_pool[str(episode)] = self.all_args.init_elo
```

**åŠŸèƒ½è§£æ:**
- **æ¨¡å‹ä¿å­˜**: ä¿å­˜Actorå’ŒCriticç½‘ç»œå‚æ•°
- **ç‰ˆæœ¬ç®¡ç†**: è‡ªå¯¹å¼ˆæ¨¡å¼ä¸‹ä¿å­˜æ¯ä¸ªepisodeçš„ç­–ç•¥ç‰ˆæœ¬
- **æ€§èƒ½è®°å½•**: ä¸ºæ–°ç­–ç•¥åˆ†é…åˆå§‹ELOè¯„åˆ†

### **9. å¯¹æ‰‹é‡ç½® `reset_opponent()`**

```python
def reset_opponent(self):
    # é€‰æ‹©æ–°çš„å¯¹æ‰‹ç­–ç•¥
    choose_opponents = []
    for policy in self.opponent_policy:
        choose_idx = self.selfplay_algo.choose(self.policy_pool)
        choose_opponents.append(choose_idx)
        policy.actor.load_state_dict(torch.load(str(self.save_dir) + f'/actor_{choose_idx}.pt'))
        policy.prep_rollout()
    
    logging.info(f" Choose opponents {choose_opponents} for training")
    
    # æ¸…ç©ºç¼“å†²åŒº
    self.buffer.clear()
    self.opponent_obs = np.zeros_like(self.opponent_obs)
    self.opponent_rnn_states = np.zeros_like(self.opponent_rnn_states)
    self.opponent_masks = np.ones_like(self.opponent_masks)
    
    # é‡ç½®ç¯å¢ƒ
    obs, share_obs = self.envs.reset()
    if self.all_args.n_choose_opponents > 0:
        self.opponent_obs = obs[:, self.num_agents // 2:, ...]
        obs = obs[:, :self.num_agents // 2:, ...]
        share_obs = share_obs[:, :self.num_agents // 2:, ...]
    
    self.buffer.obs[0] = obs.copy()
    self.buffer.share_obs[0] = share_obs.copy()
```

**åŠŸèƒ½è§£æ:**
- **å¯¹æ‰‹é€‰æ‹©**: ä½¿ç”¨è‡ªå¯¹å¼ˆç®—æ³•é€‰æ‹©æ–°çš„å¯¹æ‰‹ç­–ç•¥
- **ç­–ç•¥åŠ è½½**: å°†é€‰ä¸­çš„ç­–ç•¥åŠ è½½åˆ°å¯¹æ‰‹ç­–ç•¥å®ä¾‹ä¸­
- **ç¼“å†²åŒºæ¸…ç†**: æ¸…ç©ºç»éªŒå›æ”¾ç¼“å†²åŒº
- **ç¯å¢ƒé‡ç½®**: é‡ç½®è®­ç»ƒç¯å¢ƒå¹¶é‡æ–°åˆ†é…æ•°æ®

## ğŸ”‘ **å…³é”®è®¾è®¡ç‰¹ç‚¹**

### **1. å¤šæ™ºèƒ½ä½“æ”¯æŒ**
- æ”¯æŒå…±äº«è§‚å¯Ÿç©ºé—´
- æ™ºèƒ½ä½“é—´ç»éªŒå…±äº«
- å›¢é˜Ÿå¥–åŠ±åˆ†é…

### **2. è‡ªå¯¹å¼ˆè®­ç»ƒ**
- FSPç®—æ³•å®ç°
- ç­–ç•¥æ± ç®¡ç†
- åŠ¨æ€å¯¹æ‰‹é€‰æ‹©

### **3. å¹¶è¡Œè®­ç»ƒ**
- å¤šè¿›ç¨‹ç¯å¢ƒæ”¯æŒ
- æ‰¹é‡æ•°æ®æ”¶é›†
- é«˜æ•ˆçš„ç»éªŒå›æ”¾

### **4. æ¨¡å—åŒ–è®¾è®¡**
- æ¸…æ™°çš„èŒè´£åˆ†ç¦»
- å¯æ‰©å±•çš„ç®—æ³•æ”¯æŒ
- çµæ´»çš„é…ç½®é€‰é¡¹

## ğŸ“Š **ä½¿ç”¨åœºæ™¯**

1. **2v2ç©ºæˆ˜è®­ç»ƒ**: 4æ¶æˆ˜æ–—æœºååŒä½œæˆ˜
2. **å¤šæ™ºèƒ½ä½“åä½œ**: å­¦ä¹ å›¢é˜Ÿç­–ç•¥å’Œåè°ƒ
3. **è‡ªå¯¹å¼ˆæå‡**: é€šè¿‡å¯¹æŠ—ä¸æ–­æå‡ç­–ç•¥è´¨é‡
4. **å®æ—¶è¯„ä¼°**: å®šæœŸè¯„ä¼°ç­–ç•¥æ€§èƒ½
5. **å¯è§†åŒ–åˆ†æ**: ç”ŸæˆTacviewæ ¼å¼çš„ä½œæˆ˜è®°å½•

è¿™ä¸ªRunneræ˜¯æ•´ä¸ªå¤šæ™ºèƒ½ä½“è®­ç»ƒç³»ç»Ÿçš„æ ¸å¿ƒï¼Œå®ƒåè°ƒäº†ç¯å¢ƒäº¤äº’ã€ç­–ç•¥è®­ç»ƒã€è‡ªå¯¹å¼ˆç®¡ç†å’Œæ€§èƒ½è¯„ä¼°ç­‰æ‰€æœ‰å…³é”®åŠŸèƒ½ã€‚ 